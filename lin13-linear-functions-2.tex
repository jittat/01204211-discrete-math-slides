\include{commons}
\lecturetitle{Lecture 13a: Linear functions (II)}


\begin{frame}
  \frametitle{Review: Linear functions}

  \begin{block}{Linear functions}
    Consider vector spaces $\V$ and $\W$ over $\rf$.  A function
    $f:\V\rightarrow\W$ is \textcolor{red}{\bf linear} if
    \begin{enumerate}
    \item for all $\vect{x},\vect{y}\in\V$,
      $f(\vect{x}+\vect{y})=f(\vect{x}) + f(\vect{y})$ and
    \item for all $\alpha\in\rf$ and $\vect{x}\in\V$,
      $f(\alpha\vect{x})=\alpha f(\vect{x})$.
    \end{enumerate}
  \end{block}
\end{frame}

\begin{frame}
  \frametitle{Matrix-vector multiplication}

  Given an $m\times n$ matrix $M$ over $\rf$, consider a product
  \[
  M\vect{x}.
  \]
  Note that for the multiplication to work, $\vect{x}$ must be in
  $\rf^n$ and the result vector is in $\rf^m$.  Therefore, we can define
  a function $f: \rf^n \rightarrow \rf^m$ as
  \[
  f(\vect{x}) = M\vect{x}.
  \]
  Note that $f$ is linear because:
  \[
  f(\vect{x}+\vect{y})=M(\vect{x}+\vect{y})=M\vect{x} + M\vect{y}=
  f(\vect{x})+f(\vect{y}),
  \]
  and
  \[
  f(\alpha\vect{x})=M(\alpha\vect{x})=\alpha M\vect{x} = \alpha f(\vect{x}).
  \]
\end{frame}

\begin{frame}
  \frametitle{The converse}
  \begin{lemma}
    For any linear function $f:\rf^n \rightarrow \rf^m$, there exists an
    $m\times n$ matrix $M$ such that
    \[
    f(\vect{x}) = M\vect{x}.
    \]
  \end{lemma}
\end{frame}

\begin{frame}
  \frametitle{Example: a system of linear equations}

  Consider the following homogeneous system $A\vect{x} = \vect{0}$:
  \[
  \begin{bmatrix}
    1 & 2 & 3 & 3 & 1 \\
    3 & 4 & 7 & 5 & 3 \\
    6 & 7 & 13 & 8 & 6 \\
    2 & 4 & 6 & 14 & 6 \\
    4 & 6 & 10 & 10 & 5 
  \end{bmatrix}
  \begin{bmatrix}
    x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5
  \end{bmatrix}
  =
  \begin{bmatrix}
    0 \\ 0 \\ 0 \\ 0 \\ 0
  \end{bmatrix}
  \]

  \pause
  Let's try to solve it on Colab.
\end{frame}

\begin{frame}
  \frametitle{Example: a system of linear equations}

  Let's look at what we've got so far (after row permutation)
  \[
  \begin{bmatrix}
    1 & 2 & 3 & 3 & 1 \\
    0 & 1 & 1 & 2 & 0 \\
    0 & 0 & 0 & 2 & 1 \\
    0 & 0 & 0 & 0 & 0 \\
    0 & 0 & 0 & 0 & 0 
  \end{bmatrix}
  \begin{bmatrix}
    x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5
  \end{bmatrix}
  =
  \begin{bmatrix}
    0 \\ 0 \\ 0 \\ 0 \\ 0
  \end{bmatrix}
  \]

  \pause
  What is the rank of $A$?

  \pause
  What does nullspace of $A$ look like?
\end{frame}

\begin{frame}
  \frametitle{Example: nullspace of $A$}

  {\footnotesize
    \[
    \begin{bmatrix}
      1 & 2 & 3 & 3 & 1 \\
      0 & 1 & 1 & 2 & 0 \\
      0 & 0 & 0 & 2 & 1 \\
      0 & 0 & 0 & 0 & 0 \\
      0 & 0 & 0 & 0 & 0 
    \end{bmatrix}
    \begin{bmatrix}
      x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 \\ 0 \\ 0 \\ 0 \\ 0
    \end{bmatrix}
    \]
  }
  \vspace{0.2in}
  
  \pause
  Let's look at row 3:
  \[
  2x_4 + x_5 = 0.
  \]

  \pause
  Let's look at row 2:
  \[
  x_2 + x_3 + 2x_4 = 0.
  \]

  \pause
  Finally, let's look at row 1:
  \[
  x_1 + 2x_2 + 3x_3 + 3x_4 + x_5 = 0.
  \]

  \pause
  How many ``free'' variable that you can set?
  
\end{frame}

\begin{frame}
  \frametitle{Ranks and nullities}
\end{frame}

\begin{frame}
  \frametitle{Viewing matrix-vector multiplication as linear mapping}
  {\footnotesize
    \[
    \begin{bmatrix}
    1 & 2 & 3 & 3 & 1 \\
    3 & 4 & 7 & 5 & 3 \\
    6 & 7 & 13 & 8 & 6 \\
    2 & 4 & 6 & 14 & 6 \\
    4 & 6 & 10 & 10 & 5 
    \end{bmatrix}
    \begin{bmatrix}
      x_1 \\ x_2 \\ x_3 \\ x_4 \\ x_5
    \end{bmatrix}
    =
    \begin{bmatrix}
      0 \\ 0 \\ 0 \\ 0 \\ 0
    \end{bmatrix}
    \]
  }
  \vspace{2in}
\end{frame}

\begin{frame}
  \frametitle{Structures of linear functions}
\end{frame}

\begin{frame}
  \frametitle{Zero}

\begin{lemma}
  Consider any linear function $f:\V\rightarrow\W$.  Let $0_\V$ denote
  the zero vector in $\V$ and $0_\W$ denote the zero vector in $\W$.
  We have that linear function $f$ always maps zero to zero, i.e.,
  $f(0_\V)=0_\W$.
\end{lemma}
\begin{proof}
  First note that $0_\V=0_\V+0_\V$.  Since $f$ is linear, we have that
  \[
  f(0_\V)=f(0_\V+0_\V)=f(0_\V)+f(0_\V).
  \]
  Subtracting $f(0_\V)$ from both sides, we conclude that
  \[
  0_\W=f(0_\V).
  \]
\end{proof}
\end{frame}

\begin{frame}
\frametitle{One-to-one linear functions and Onto linear functions}

\begin{block}{One-to-one and onto functions}
Consider a function $f:D\rightarrow R$ (i.e., the domain of $f$ is $D$
and its range is $R$).

\begin{itemize}
\item
  Function $f$ is \textcolor{red}{\bf one-to-one} (or {\bf injective}) if for all
  $x,y\in D$, $f(x)=f(y)$ implies that $x=y$.
\item
  Function $f$ is \textcolor{red}{\bf onto} (or {\bf surjective}) if for all $x\in
  R$, there exists $y\in D$ such that $f(y)=x$.
\end{itemize}
\end{block}

\vspace{0.2in}

For this course, we consider only linear functions; therefore,
we consider $f:\V\rightarrow\W$, where $\V$ and $\W$ are vector spaces.
\end{frame}

\begin{frame}
\frametitle{One-to-one linear functions}

Suppose that $f$ is not one-to-one, \pause i.e., there exists a pair
$x,y\in\V$ such that $x\neq y$ and $f(x)=f(y)$. \pause Since $f$ is linear,
we know that
\[
f(x-y)=f(x)-f(y)=0.
\]
\pause
Since $x\neq y$, $x-y\neq 0$ and we have that there exists a non-zero
element $z=x-y$ that $f$ maps to $0$.  \pause The contraposition of this fact
is as follows.

If the only element in $\V$ that $f$ maps to $0_\W$ is
$0_\V$, $f$ is one-to-one (or injective).
\end{frame}

\begin{frame}

Because the set of elements that $f$ maps to zero is very important,
we have a name for it.

\begin{block}{Definition (Kernel)}
The \textcolor{red}{\bf kernel} of $f$, denoted by $\ker f$, is the
set of all elements that $f$ maps to zero, i.e.,
\[
\ker f = \{\vv\in\V : f(\vv) = 0_\V\}.
\]
\end{block}

We can now restate the condition for $f$ to be one-to-one using this
concept.

\begin{lemma}
  A linear function $f$ is one-to-one, if and only if $\ker f=\{0\}$.
\end{lemma}

\end{frame}

\begin{frame}
\frametitle{The kernel is also a vector space}

\begin{lemma}
  $\ker f$ is a vector space.
\end{lemma}
\begin{proof}
  \pause
  First note that $f(0)=0$; thus $0\in\ker f$.

  \pause
  Suppose that $x\in\ker f$, i.e., $f(x)=0$.  Note that for any scalar
  $\alpha$,
  \[
  f(\alpha x) = \alpha f(x) = \alpha 0 = 0.
  \]

  \pause
  Also suppose $y\in\ker f$.  We have that
  \[
  f(x+y) = f(x) + f(y) = 0 + 0 = 0.
  \]
\end{proof}
\end{frame}

\begin{frame}
\frametitle{Onto linear functions}

\begin{block}{Definition (Image)}
For any function $g$, its \textcolor{red}{\bf image}, denoted by $\img g$, is the set of all
elements that $g$ maps to, i.e.,
\[
\img g = \{y : \mbox{there exists $x$ such that $g(x)=y$}\}.
\]
\end{block}

\end{frame}

\begin{frame}
\frametitle{The image is also a vector space}

\begin{lemma}
  The image of linear function $f$, $\img f$, is a vector space.
\end{lemma}
\begin{proof}
  \pause
  Since $f(0_\V)=0_\W$, $0_\W\in\img f$.

  \pause
  Consider $y\in\img f$. We have that there exists $x$ such that
  $f(x)=y$.  Consider any scalar $\alpha$.  We know that $\alpha
  y\in\img f$ because $f(\alpha x) = \alpha f(x) =\alpha y$.

  \pause
  Consider, also, $y'\in\img f$.  Let $x'$ be such that $f(x')=y'$.
  Since $y'\in\img f$, we know that $x'$ exists.  We have that
  \[
  f(x+x')=f(x)+f(x')=y+y'.
  \]
  This implies that $y+y'\in\img f$.
\end{proof}
\end{frame}

\begin{frame}
\frametitle{Kernels and images}

\begin{theorem}[Kernel-Image Theorem]
  Consider a linear function $f:\V\rightarrow\W$.  We have that
  \[
  \dim \V = \dim\ker f + \dim\img f.
  \]
\end{theorem}
\end{frame}

\begin{frame}
  \frametitle{Completing the basis}
  
  \begin{lemma}
    For a set of linearly independent vectors
    \[
    \uv_1,\uv_2,\ldots,\uv_k
    \]
    in $\V$ with basis $B=\{\vv_1,\vv_2,\ldots,\vv_n\}$
    (where $k\leq n$), there exists a set of vectors
    $\vect{w}_1,\vect{w}_2,\ldots,\vect{w}_{n-k}\in B$ such
    that
    \[
    \{
    \uv_1,\uv_2,\ldots,\uv_k,
    \vect{w}_1,\vect{w}_2,\ldots,\vect{w}_{n-k}
    \}
    \]
    is also a basis for $\V$.
  \end{lemma}

  \pause

  \begin{proof}
    Use the morphing lemma.
  \end{proof}
\end{frame}

\begin{frame}
\begin{theorem}[Kernel-Image Theorem]
  For a linear function $f:\V\rightarrow\W$,
  $\dim \V = \dim\ker f + \dim\img f.$
\end{theorem}

\begin{proof}[Proof of Kernel-Image Theorem (1)]
  \pause
  Let $n=\dim\V$ and $k=\dim\ker f$.  Our goal is to show that
  $\dim\img f=n-k$.

  \pause
  Since $\ker f$ is a vector space, there is a basis
  $B=\{\vv_1,\vv_2,\ldots,\vv_k\}$.
  \pause
  From the previous slide, we can
  find other $n-k$ vectors
  $\vect{w}_1,\vect{w}_2,\ldots,\vect{w}_{n-k}$ to extend $B$ to be a
  basis $S$ for $\V$, i.e., we have that
  \[
  S =
  \{\vv_1,\vv_2,\ldots,\vv_k,\vect{w}_1,\vect{w}_2,\ldots,\vect{w}_{n-k}\}
  \]
  is a basis for $\V$.
\end{proof}
\end{frame}

\begin{frame}
  \begin{proof}[Proof of Kernel-Image Theorem (2)]
    \pause
  Consider any $\uv\in\V$.  We can write $\uv$ as
  \[
  \alpha_1\vv_1 + \alpha_2\vv_2 + \cdots + \alpha_k\vv_k +
  \beta_1\vect{w}_1 + \beta_2\vect{w}_2 + \cdots +
  \beta_{n-k}\vect{w}_{n-k},
  \]
  because $S$ is a basis.
  \pause
  Consider $f(\uv)$.  We have that
  \begin{eqnarray*}
  f(\uv) & = & 
  f(\alpha_1\vv_1 + \cdots + \alpha_k\vv_k +
  \beta_1\vect{w}_1 + \cdots +
  \beta_{n-k}\vect{w}_{n-k}) \\
  \pause
  & = & 
  f(\alpha_1\vv_1) + \cdots + f(\alpha_k\vv_k) +
  f(\beta_1\vect{w}_1) + \cdots +
  f(\beta_{n-k}\vect{w}_{n-k}) \\
  \pause
  & = & 
  f(\beta_1\vect{w}_1) + f(\beta_2\vect{w}_2) + \cdots +
  f(\beta_{n-k}\vect{w}_{n-k}) \\
  \pause
  & = & 
  \beta_1f(\vect{w}_1) + \beta_2f(\vect{w}_2) + \cdots +
  \beta_{n-k}f(\vect{w}_{n-k})
  \end{eqnarray*}
  {\tiny (Note that the second step follows because $\vv_i\in\ker f$.  Other
  steps use the fact that $f$ is linear.)}

  This calculation shows that \pause an image of $f$ can be written as a
  linear combination of $f(\vect{w}_1),\ldots,f(\vect{w}_{n-k})$.
  That is
  \[
  \img f = \vspan\; \{f(\vect{w}_1),\ldots,f(\vect{w}_{n-k})\}.
  \]
  \end{proof}
\end{frame}

\begin{frame}
  \begin{proof}[Proof of Kernel-Image Theorem (3)]
    {\small
      \pause
      Let $S'=\{f(\vect{w}_1),\ldots,f(\vect{w}_{n-k})\}$.  If we can show
      that $S'$ is a basis for $\img f$, we are done because that would
      imply that $\dim\img f=n-k$ as required.

      \pause
      We already know that $S'$ spans $\img f$.
      \pause To show that $S'$ is a
      basis we still need to show that $S'$ is linearly independent.

      \pause
      Suppose that there exist $\beta_1,\ldots,\beta_{n-k}$ such that
      \[
      \beta_1f(\vect{w}_1) + \beta_2f(\vect{w}_2) + \cdots +
      \beta_{n-k}f(\vect{w}_{n-k}) = 0_\W.
      \]
      \pause
      Since $f$ is linear we know that
      \begin{eqnarray*}
        0_\W &=&
        \beta_1f(\vect{w}_1) + \beta_2f(\vect{w}_2) + \cdots +
        \beta_{n-k}f(\vect{w}_{n-k})\\
        &=&
        f(\beta_1\vect{w}_1) + f(\beta_2\vect{w}_2) + \cdots +
        f(\beta_{n-k}\vect{w}_{n-k})\\
        &=&
        f(\beta_1\vect{w}_1 + \beta_2\vect{w}_2 + \cdots +
        \beta_{n-k}\vect{w}_{n-k}),
      \end{eqnarray*}
      i.e., $\beta_1\vect{w}_1 + \beta_2\vect{w}_2 + \cdots +
      \beta_{n-k}\vect{w}_{n-k}$ is in $\ker f$.
    }
  \end{proof}
\end{frame}

\begin{frame}
  \begin{proof}[Proof of Kernel-Image Theorem (4)]
    {\small
      Suppose that some $\beta_i\neq 0$.

      Since
      \[
      \beta_1\vect{w}_1 + \beta_2\vect{w}_2 + \cdots +
      \beta_{n-k}\vect{w}_{n-k} \in \ker f,
      \]
      we know that it is a linear combination of vectors from $B$, as
      $B$ is a basis for vector space $\ker f$.

      \pause From here, we can reach a contradiction using the fact
      that vectors in $S$ are linearly independent.

      \pause
      Therefore, we conclude that all $\beta_1,\ldots,\beta_{n-k}$
      must be 0.  Hence,
      $S'=\{f(\vect{w}_1),\ldots,f(\vect{w}_{n-k})\}$ is linearly
      independent as needed.
    }
  \end{proof}
\end{frame}

\begin{frame}
\frametitle{Direct sum (optional)}

Consider two subspaces $\V$ and $\W$ of a vector space ${\mathcal Z}$.
If $\V\cap\W=\{0\}$, we can define their {\em direct sum} to be
another vector space $\V\oplus\W$ as
\[
\V\oplus\W = \{\vv+\uv : \vv\in\V, \uv\in\W\}.
\]
Note, again, that $\V\oplus\W$ is defined only when $\V\cap\W=\{0\}$.
\end{frame}

